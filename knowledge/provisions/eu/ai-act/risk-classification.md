# EU AI Act — Classification of AI Systems as High-Risk

**Law**: Regulation (EU) 2024/1689 (EU Artificial Intelligence Act)
**Articles**: Articles 6-7, Annex III
**Effective**: 2 August 2026 (high-risk obligations apply)

## Overview

The EU AI Act establishes a risk-based classification framework. AI systems not
falling under prohibited practices (Article 5) are classified based on their
intended purpose and potential impact. High-risk classification triggers
extensive compliance obligations under Articles 8-15.

## Classification Framework

### Article 6(1) — High-Risk via Product Safety Legislation

An AI system is high-risk if it is:
1. A product, or a safety component of a product, covered by Union harmonisation
   legislation listed in Annex I, AND
2. Required to undergo third-party conformity assessment under that legislation.

**Annex I sectors include**: machinery, toys, lifts, equipment in explosive
atmospheres, radio equipment, pressure equipment, cableways, personal protective
equipment, medical devices, in vitro diagnostic devices, civil aviation,
motor vehicles, marine equipment, rail interoperability.

### Article 6(2) — High-Risk via Annex III

An AI system is high-risk if it falls within one of the use-case areas listed
in Annex III, UNLESS it does not pose a significant risk of harm to health,
safety, or fundamental rights (the "filter" exception in Article 6(3)).

### Article 6(3) — The Significant Risk Filter

An AI system listed in Annex III is NOT considered high-risk if it does not
pose a significant risk of harm. An AI system is deemed NOT to pose such a
risk if any of the following apply:
- It performs a narrow procedural task
- It improves the result of a previously completed human activity
- It detects decision-making patterns without replacing or influencing human
  assessment
- It performs a preparatory task to an assessment relevant to the use cases
  in Annex III

This exception does NOT apply if the AI system performs profiling of natural
persons.

## Annex III — High-Risk Use-Case Areas

### 1. Biometrics (to the extent permitted)

- Remote biometric identification systems (not real-time in public spaces for
  law enforcement, which is prohibited)
- Biometric categorisation systems based on sensitive or protected attributes
- Emotion recognition systems

### 2. Critical Infrastructure

- AI systems intended for use as safety components in the management and
  operation of critical digital infrastructure, road traffic, or water, gas,
  heating, and electricity supply

### 3. Education and Vocational Training

- AI systems intended to determine access to or admission to educational and
  vocational training institutions at all levels
- AI systems intended to evaluate learning outcomes, including when used to
  steer the learning process
- AI systems intended to assess the appropriate level of education for an
  individual and influence the level of education and training they will
  receive or be able to access
- AI systems intended to monitor and detect prohibited behaviour of students
  during tests

### 4. Employment, Workers Management, and Access to Self-Employment

- AI systems intended to be used for recruitment or selection, in particular
  to place targeted job advertisements, to analyse and filter job applications,
  and to evaluate candidates
- AI systems intended to make decisions affecting the terms of work-related
  relationships, promotion, termination, task allocation based on individual
  behaviour or personal traits, or monitoring and evaluation of worker
  performance and behaviour

### 5. Access to and Enjoyment of Essential Private Services and Public Services

- AI systems intended to evaluate creditworthiness or credit scoring (except
  for detecting financial fraud)
- AI systems intended to be used for risk assessment and pricing in life and
  health insurance
- AI systems intended to evaluate and classify emergency calls, including
  prioritisation of dispatching
- AI systems intended to be used for assessing eligibility for public
  assistance benefits and services, granting, reducing, revoking, or
  reclaiming such benefits and services

### 6. Law Enforcement

- AI systems intended to be used by or on behalf of law enforcement for
  individual risk assessments to assess the risk of a natural person
  offending or re-offending (not solely based on profiling, which is
  prohibited under Art. 5)
- AI systems intended to be used as polygraphs or similar tools
- AI systems intended to evaluate the reliability of evidence in criminal
  investigations or prosecutions
- AI systems intended to assess the risk of a natural person becoming a
  victim of criminal offences
- AI systems for crime analytics regarding natural persons

### 7. Migration, Asylum, and Border Control Management

- AI systems intended to assist in the examination of applications for
  asylum, visa, or residence permits
- AI systems intended to be used for polygraphs or similar tools in the
  context of migration
- AI systems intended to be used for risk assessments regarding irregular
  migration
- AI systems intended to assist competent authorities for the examination
  of immigration or asylum applications, including with regard to assessing
  the relevant security risk

### 8. Administration of Justice and Democratic Processes

- AI systems intended to be used by a judicial authority or on their behalf
  to assist in researching and interpreting facts and the law and in applying
  the law to concrete facts, or to be used in alternative dispute resolution
- AI systems intended to be used to influence the outcome of an election or
  referendum or the voting behaviour of natural persons

## Limited-Risk Category

AI systems that interact directly with natural persons have transparency
obligations (Articles 50-52) but do not need to comply with the full
high-risk requirements. These include:
- Chatbots and conversational AI (must disclose AI interaction)
- Emotion recognition systems (must disclose)
- AI-generated content / deepfakes (must label)
- Biometric categorisation systems (must disclose)

## Minimal-Risk Category

AI systems that do not fall into prohibited, high-risk, or limited-risk
categories are minimal risk and have no mandatory requirements beyond
voluntary codes of conduct. Examples include:
- Spam filters
- Recommendation engines for entertainment
- Inventory management systems
- AI-enabled video games

## Citations

- Regulation (EU) 2024/1689, Articles 6-7
- Regulation (EU) 2024/1689, Annex III
- Recitals 46-65
