# EU AI Act — Requirements for High-Risk AI Systems

**Law**: Regulation (EU) 2024/1689 (EU Artificial Intelligence Act)
**Articles**: Articles 8-15
**Effective**: 2 August 2026

## Overview

High-risk AI systems must comply with a comprehensive set of requirements
before being placed on the EU market or put into service. These requirements
ensure that high-risk AI systems are safe, respect fundamental rights, and
can be effectively overseen by humans.

## Article 8 — Compliance with Requirements

High-risk AI systems shall comply with the requirements laid down in Articles
9-15, taking into account the intended purpose, the generally acknowledged
state of the art, and the specific context in which the AI system is used.

## Article 9 — Risk Management System

Providers of high-risk AI systems must establish, implement, document, and
maintain a risk management system. This must include:

1. **Identification and analysis** of known and reasonably foreseeable risks
2. **Estimation and evaluation** of risks that may emerge when the system is
   used in accordance with its intended purpose and under conditions of
   reasonably foreseeable misuse
3. **Evaluation of other risks** based on post-market monitoring data
4. **Adoption of appropriate risk management measures** addressing identified
   risks

The risk management system shall be a continuous iterative process throughout
the entire lifecycle of the AI system.

Testing shall be performed to identify the most appropriate risk management
measures, including testing under real-world conditions where appropriate.

## Article 10 — Data and Data Governance

Training, validation, and testing datasets must meet quality criteria:

1. **Relevant design choices** for datasets must be documented
2. **Data governance and management practices** must address: data collection,
   data preparation, relevant assumptions, prior assessment of availability,
   quantity, and suitability
3. **Training datasets** must be relevant, sufficiently representative, and
   to the best extent possible free of errors and complete
4. **Bias examination**: datasets must take into account the specific
   geographical, contextual, behavioural, or functional setting within which
   the AI system is intended to be used
5. **Special categories of personal data** (Article 9(1) GDPR) may be
   processed for bias monitoring to the extent strictly necessary

## Article 11 — Technical Documentation

Providers must draw up technical documentation BEFORE the system is placed
on the market. The documentation must demonstrate compliance with Articles
8-15 and provide sufficient information for authorities to assess compliance.

Required contents include system description, development process, monitoring
and functioning details, risk management documentation, and relevant changes
throughout the lifecycle.

## Article 12 — Record-Keeping (Logging)

High-risk AI systems must be designed to automatically record events (logs)
throughout their lifetime. Logging capabilities must ensure:

1. Recording of the period of each use (start and end date and time)
2. The reference database against which input data has been checked
3. Input data for which the search has led to a match
4. The identification of natural persons involved in the verification of results

Logs must be kept for a period appropriate to the intended purpose of the
high-risk AI system, of at least 6 months (unless otherwise provided by
applicable Union or national law).

## Article 13 — Transparency and Provision of Information to Deployers

High-risk AI systems must be designed to ensure their operation is sufficiently
transparent for deployers to interpret and use output appropriately.

Instructions for use must include:
1. Identity and contact details of the provider
2. Characteristics, capabilities, and limitations of performance
3. Intended purpose and any known or foreseeable misuse
4. Changes to the system (pre-determined by the provider)
5. Human oversight measures and technical measures to facilitate interpretation
6. Expected lifetime and maintenance measures
7. Computational and hardware resources needed

## Article 14 — Human Oversight

High-risk AI systems must be designed to be effectively overseen by natural
persons during their period of use. Human oversight aims to prevent or
minimise risks to health, safety, or fundamental rights.

Human oversight measures must enable the individual(s) overseeing to:
1. Fully understand the capacities and limitations of the AI system
2. Be aware of and able to monitor for automation bias
3. Correctly interpret the AI system's output
4. Decide not to use the AI system, disregard, override, or reverse its output
5. Intervene or interrupt the system via a "stop" button or similar procedure

For AI systems identified as high-risk in Annex III, point 1(a) (biometric
identification), human oversight must include at minimum two natural persons
verifying results before action is taken.

## Article 15 — Accuracy, Robustness, and Cybersecurity

High-risk AI systems must achieve an appropriate level of:

1. **Accuracy**: appropriate levels of accuracy for their intended purpose,
   declared in instructions for use
2. **Robustness**: resilient to errors, faults, or inconsistencies, including
   in relation to inputs or environments that may affect outputs
3. **Cybersecurity**: resilient against attempts by unauthorised third parties
   to alter use or performance by exploiting system vulnerabilities

Technical redundancy solutions may include back-up plans and fail-safe
mechanisms.

## Conformity Assessment (Articles 43-44)

High-risk AI systems must undergo conformity assessment before being placed
on the market:

- **Annex III systems (except biometrics)**: self-assessment by the provider,
  based on internal control (Annex VI)
- **Biometric identification systems**: third-party assessment by a notified
  body (Annex VII)
- **Annex I systems (product safety)**: follow the conformity assessment
  procedure of the relevant sectoral legislation

After successful conformity assessment, providers affix the CE marking and
register the system in the EU database.

## Key Obligations for Providers

1. Implement a quality management system (Article 17)
2. Maintain technical documentation (Article 18)
3. Retain automatically generated logs (Article 19)
4. Ensure conformity assessment (Article 43)
5. Affix CE marking (Article 48)
6. Register in the EU database (Article 49)
7. Implement post-market monitoring (Article 72)
8. Report serious incidents (Article 73)

## Citations

- Regulation (EU) 2024/1689, Articles 8-15
- Regulation (EU) 2024/1689, Articles 43-44, 48-49, 72-73
- Regulation (EU) 2024/1689, Annexes IV, VI, VII
- Recitals 66-83
