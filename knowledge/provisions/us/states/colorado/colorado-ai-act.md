# Colorado AI Act (SB 24-205)

**Authority**: Colorado Legislature; Colorado Attorney General
**Legal Basis**: SB 24-205, Colorado Revised Statutes ยง 6-1-1701 et seq.
**Status**: Signed into law May 2024; effective February 1, 2026

## Overview

The Colorado AI Act is one of the most comprehensive state-level AI regulations
in the United States. It imposes obligations on both **developers** (entities
that design, code, or produce AI systems) and **deployers** (entities that use
AI systems) when high-risk AI systems are used to make or substantially
contribute to **consequential decisions** about consumers. The law focuses on
preventing algorithmic discrimination and ensuring transparency and
accountability in AI-driven decision-making.

## High-Risk AI System Definition

An AI system is considered **high-risk** if it makes, or is a substantial
factor in making, a **consequential decision** concerning a consumer. A
consequential decision is one that has a material legal or similarly significant
effect on a consumer's access to or the cost, terms, or availability of:

- **Education**: Enrollment, financial aid, credentialing, certification
- **Employment**: Hiring, termination, compensation, promotion, assignment
- **Financial Services**: Lending, credit, insurance, housing finance
- **Housing**: Rental, purchase, terms of tenancy
- **Healthcare**: Clinical decision support, coverage, cost of services
- **Insurance**: Underwriting, pricing, coverage decisions, claims
- **Government Services**: Eligibility for public benefits, licensing,
  permits, access to government programs
- **Legal Services**: Decisions that affect legal rights or obligations

## Developer Obligations

Developers of high-risk AI systems must:

### Documentation and Disclosure

- Provide deployers with a **general description** of the types of
  consequential decisions for which the system was designed
- Disclose **known limitations** of the system, including known risks of
  algorithmic discrimination
- Provide a **high-level summary** of the training data used, including
  known sources of bias
- Publish a **statement** on the developer's website summarizing the types
  of high-risk AI systems the developer has developed and how risks of
  algorithmic discrimination are managed

### Technical Measures

- Make available documentation reasonably necessary for deployers to conduct
  impact assessments
- Provide deployers with a description of the data used to train the system,
  the purpose and intended uses, and the system's known performance metrics
- Provide a point of contact for deployer inquiries

## Deployer Obligations

Deployers of high-risk AI systems must:

### Risk Management Policy

Implement a **risk management policy and program** that is reasonable in
light of the deployer's size, complexity, and the nature of the AI system.
The policy must include:
- Identification and documentation of known risks of algorithmic discrimination
- Reasonable steps to mitigate identified risks
- Ongoing monitoring of the AI system for algorithmic discrimination

### Impact Assessment

Complete an **impact assessment** for each high-risk AI system before
deployment and annually thereafter. The assessment must include:
- A description of the purpose, intended use cases, and deployment context
- An analysis of whether the system poses risks of algorithmic discrimination
- The categories of data processed and the outputs produced
- Transparency measures provided to consumers
- Post-deployment monitoring plans
- A description of how the system is evaluated for performance and fairness

Impact assessments must be retained for at least three years and provided
to the Attorney General upon request.

### Consumer Notice and Transparency

- Provide consumers with **notice** that a high-risk AI system is being used
  to make or substantially contribute to a consequential decision
- Provide a **description** of the AI system and how it contributes to the
  decision in plain language
- Provide consumers the opportunity to **correct** inaccurate personal data
  the system used in the decision
- Provide consumers the opportunity to **appeal** an adverse consequential
  decision, including access to human review

### Human Oversight

- Ensure that consequential decisions are subject to appropriate human
  oversight, proportionate to the risk of harm
- Human reviewers must have the authority, competence, and resources to
  override the AI system's output

## Algorithmic Discrimination

### Definition

Algorithmic discrimination means any condition in which the use of an AI
system results in an unlawful differential treatment or impact that
disfavors a consumer based on:
- Race, color, ethnicity, or national origin
- Sex, gender identity, or sexual orientation
- Religion
- Age (40 and older)
- Disability
- Veteran status

### Affirmative Defense

A deployer has an affirmative defense to an allegation of violating the Act
if the deployer:
- Discovered and cured the violation within 90 days
- Complied with the risk management, impact assessment, and notice requirements
- Complied with any applicable voluntary frameworks (e.g., NIST AI RMF)

## Enforcement

- **Exclusive enforcement** by the Colorado Attorney General under the
  Colorado Consumer Protection Act
- No private right of action
- The Attorney General must provide 60-day notice before bringing an action,
  allowing the deployer to cure the violation
- Penalties under the Colorado Consumer Protection Act (up to $20,000 per
  violation)
- The Attorney General may promulgate additional rules

## Interaction with Other Laws

- The Colorado AI Act does not limit rights or obligations under federal
  anti-discrimination law (Title VII, ADA, ADEA, ECOA, Fair Housing Act)
- CCPA/CPRA obligations apply independently for businesses also subject to
  California law
- The Act does not preempt local ordinances

## Citations

- SB 24-205, Colorado Revised Statutes ยง 6-1-1701 et seq.
- Colorado Consumer Protection Act, C.R.S. ยง 6-1-101 et seq.
- NIST AI Risk Management Framework (referenced as voluntary compliance framework)
